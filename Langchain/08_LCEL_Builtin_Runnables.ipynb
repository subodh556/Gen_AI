{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv , find_dotenv\n",
    "\n",
    "_ =load_dotenv(find_dotenv())\n",
    "\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "chatm = ChatGroq(\n",
    "    model=\"llama3-70b-8192\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runnable Passthrough\n",
    "\n",
    ". It does not do anything to the input data. Let's see it in a very simple example: a\n",
    "  chain with just RunnablePassthrough() will output the original input wihout any modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'subodh'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "chain = RunnablePassthrough()\n",
    "\n",
    "chain.invoke(\"subodh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runnable Lambda\n",
    "\n",
    ". To use a custom function inside a LCEL chain we need to wrap it up with RunnableLambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'subodhovich'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def russian_lastname(name:str):\n",
    "    return f\"{name}ovich\"\n",
    "\n",
    "\n",
    "chain = RunnablePassthrough() | RunnableLambda(russian_lastname)\n",
    "\n",
    "chain.invoke(\"subodh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runnable Parallel\n",
    "\n",
    "used to run the tasks in parallel. most useful runnable.\n",
    "in the following chain, runnable parallel is going to run these tow tasks in parllel:\n",
    "    operation_a will use runnablepassthrough\n",
    "    operation_b will use runnablelambda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'operation_a': 'subodh', 'operation_b': 'subodhovich'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "chain = RunnableParallel(\n",
    "    {\n",
    "        \"operation_a\":RunnablePassthrough(),\n",
    "        \"operation_b\":RunnableLambda(russian_lastname)\n",
    "    }\n",
    ")\n",
    "\n",
    "chain.invoke(\"subodh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I think you meant \"Messiovich\" doesn\\'t exist, but I\\'m assuming you meant Lionel Messi!\\n\\nHere\\'s a curious fact about Messi:\\n\\n**Messi\\'s growth hormone deficiency**: When Messi was 11 years old, he was diagnosed with a growth hormone deficiency, which meant his body wasn\\'t producing enough growth hormone. This condition can affect growth and development in children. FC Barcelona, his future club, agreed to pay for his treatment, which involved daily injections of growth hormone for several years. This treatment helped Messi grow to his adult height of 5 feet 7 inches (170 cm), and the rest, as they say, is history!'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a curious fact about {soccer_player}\")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "def russian_lastname_from_dictionay(person):\n",
    "    return person[\"name\"] + \"ovich\"\n",
    "\n",
    "chain = RunnableParallel(\n",
    "    {\n",
    "        \"operation_a\":RunnablePassthrough(),\n",
    "        \"soccer_player\":RunnableLambda(russian_lastname_from_dictionay),\n",
    "        \"operation_b\":RunnablePassthrough(),\n",
    "    }\n",
    ") | prompt | chatm | output_parser\n",
    "\n",
    "chain.invoke({\"name\":\"messi\",\"name2\":\"Abram\"\n",
    "              })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advance use of Runnable Parrallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate   \n",
    "from langchain_core.runnables import RunnableParallel,RunnablePassthrough\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = FAISS.from_texts([\"subodh focusses on work , AI,ML,DL.\"],embedding=OpenAIEmbeddings())\n",
    "\n",
    "retreiver = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "question:{question}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chatm = ChatGroq( model=\"llama3-70b-8192\")\n",
    "\n",
    "chain= (\n",
    "    RunnableParallel({\"context\":retreiver,\"question\":RunnablePassthrough()})\n",
    "    | prompt\n",
    "    | chatm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "chain.invoke(\"what is subodh?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the example of itemgetter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate   \n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = FAISS.from_texts([\"subodh focusses on work , AI,ML,DL.\"],embedding=OpenAIEmbeddings())\n",
    "\n",
    "retreiver = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "question:{question}\n",
    "\n",
    "Answer in the following language:{language}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\":itemgetter(\"question\") |  retreiver,\n",
    "        \"question\":itemgetter(\"question\") ,\n",
    "        \"language\":itemgetter(\"language\")\n",
    "    }\n",
    "    | prompt\n",
    "    | chatm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\":\"what is subodh?\",\"language\":\"english\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
